{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Gold Semantic Layer\n\nThis notebook transforms raw financial article data into a star schema with:\n- **Fact Table**: Company event signals with metrics\n- **Dimension Tables**: Companies and event types\n\nThe pipeline is idempotent and can be safely re-run."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Configuration"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Catalog configuration\nCATALOG_NAME = \"stage-us-east-1-jngai-dev\"\nSOURCE_NAMESPACE = \"financial_articles\"\nSOURCE_TABLE = \"fmp_articles\"\nTARGET_NAMESPACE = \"gold_semantic_layer\"\n\n# Table names\nFACT_TABLE = \"fact_company_event_signal\"\nDIM_COMPANY_TABLE = \"dim_company\"\nDIM_EVENT_TYPE_TABLE = \"dim_event_type\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Startup Cells"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os\nos.environ['DataZoneProjectId'] = '4hril53mejrp2f'\nos.environ['DataZoneDomainId'] = 'dzd-5w47wlphwxsdev'\nos.environ['DataZoneEnvironmentId'] = 'dcza2emsroy8br'\nos.environ['DataZoneDomainRegion'] = 'us-east-1'\n\n_resource_metadata = None\n\ndef _get_resource_metadata():\n    global _resource_metadata\n    if _resource_metadata is None:\n        _resource_metadata = {\n            \"AdditionalMetadata\": {\n                \"DataZoneProjectId\": \"4hril53mejrp2f\",\n                \"DataZoneDomainId\": \"dzd-5w47wlphwxsdev\",\n                \"DataZoneEnvironmentId\": \"dcza2emsroy8br\",\n                \"DataZoneDomainRegion\": \"us-east-1\",\n            }\n        }\n    return _resource_metadata\n\nmetadata = _get_resource_metadata()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from typing import Optional\n\ndef _set_logging(log_dir: str, log_file: str, log_name: Optional[str] = None):\n    import os\n    import logging\n    from logging.handlers import RotatingFileHandler\n\n    level = logging.INFO\n    max_bytes = 5 * 1024 * 1024\n    backup_count = 5\n\n    try:\n        os.makedirs(log_dir, exist_ok=True)\n    except Exception:\n        log_dir = \"/tmp/kernels/\"\n\n    os.makedirs(log_dir, exist_ok=True)\n    log_path = os.path.join(log_dir, log_file)\n\n    logger = logging.getLogger() if not log_name else logging.getLogger(log_name)\n    logger.handlers = []\n    logger.setLevel(level)\n\n    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n    fh = RotatingFileHandler(filename=log_path, maxBytes=max_bytes, backupCount=backup_count, encoding=\"utf-8\")\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n    logger.info(f\"Logging initialized for {log_name}.\")\n\n_set_logging(\"/var/log/computeEnvironments/kernel/\", \"kernel.log\")\n_set_logging(\"/var/log/studio/data-notebook-kernel-server/\", \"metrics.log\", \"metrics\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import logging\nfrom sagemaker_studio import ClientConfig, sqlutils, sparkutils, dataframeutils\n\nlogger = logging.getLogger(__name__)\nlogger.info(\"Initializing sparkutils\")\nspark = sparkutils.init()\nlogger.info(\"Finished initializing sparkutils\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def _reset_os_path():\n    try:\n        import os\n        import logging\n        logger = logging.getLogger(__name__)\n        logger.info(\"---------Before------\")\n        logger.info(\"CWD: %s\", os.getcwd())\n        logger.info(\"stat('.'): %s %s\", os.stat('.').st_dev, os.stat('.').st_ino)\n        logger.info(\"stat('/home/sagemaker-user'): %s %s\", os.stat('/home/sagemaker-user').st_dev, os.stat('/home/sagemaker-user').st_ino)\n        os.chdir(\"/home/sagemaker-user\")\n        logger.info(\"---------After------\")\n        logger.info(\"CWD: %s\", os.getcwd())\n        logger.info(\"stat('.'): %s %s\", os.stat('.').st_dev, os.stat('.').st_ino)\n        logger.info(\"stat('/home/sagemaker-user'): %s %s\", os.stat('/home/sagemaker-user').st_dev, os.stat('/home/sagemaker-user').st_ino)\n    except Exception as e:\n        logger.exception(f\"Failed to reset working directory: {e}\")\n\n_reset_os_path()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Import Libraries"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from pyspark.sql import functions as F, DataFrame\nfrom pyspark.sql.window import Window\nimport re",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Helper Functions"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def classify_event_type(title: str) -> str:\n    \"\"\"Classify event type based on article title.\"\"\"\n    title_lower = title.lower()\n    \n    if \"earnings\" in title_lower or \"quarter\" in title_lower:\n        return \"earnings\"\n    elif \"price target\" in title_lower:\n        return \"price_target\"\n    elif \"rating\" in title_lower or \"buy rating\" in title_lower:\n        return \"rating\"\n    elif \"product\" in title_lower or \"launch\" in title_lower:\n        return \"product\"\n    else:\n        return \"other\"\n\ndef extract_price_target_delta(content: str) -> float:\n    \"\"\"Extract price target percentage change from article content.\"\"\"\n    matches = re.findall(r\"\\$(\\d+\\.?\\d*)\", content)\n    if len(matches) >= 2:\n        try:\n            target = float(matches[0])\n            price = float(matches[1])\n            if price != 0:\n                return (target - price) / price\n        except (ValueError, ZeroDivisionError):\n            pass\n    return None\n\n# Register UDFs\nclassify_event_type_udf = F.udf(classify_event_type)\nextract_price_target_delta_udf = F.udf(extract_price_target_delta, \"double\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def write_table_idempotent(df: DataFrame, catalog: str, namespace: str, table_name: str) -> None:\n    \"\"\"Write DataFrame to Iceberg table with overwrite for idempotency.\n    \n    Args:\n        df: Spark DataFrame to write\n        catalog: Catalog name\n        namespace: Namespace/schema name\n        table_name: Table name\n    \"\"\"\n    full_table_name = f\"`{catalog}`.`{namespace}`.`{table_name}`\"\n    \n    # Check if table exists\n    table_exists = spark.catalog.tableExists(f\"{catalog}.{namespace}.{table_name}\")\n    \n    if table_exists:\n        # Overwrite existing data\n        df.writeTo(full_table_name).overwritePartitions()\n        print(f\"✓ Overwrote table: {full_table_name}\")\n    else:\n        # Create new table\n        df.writeTo(full_table_name).using(\"iceberg\").create()\n        print(f\"✓ Created table: {full_table_name}\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Extract Data"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Read source data\nsource_table = f\"`{CATALOG_NAME}`.`{SOURCE_NAMESPACE}`.`{SOURCE_TABLE}`\"\ndf_raw = spark.read.table(source_table)\n\nprint(f\"Loaded {df_raw.count()} records from {source_table}\")\ndf_raw.printSchema()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Transform Data"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Base transformations\ndf_transformed = (df_raw\n    # Parse datetime\n    .withColumn(\"event_datetime\", F.to_timestamp(\"date\"))\n    .withColumn(\"event_date\", F.to_date(\"event_datetime\"))\n    \n    # Extract ticker and exchange\n    .withColumn(\"exchange\", F.split(\"tickers\", \":\").getItem(0))\n    .withColumn(\"ticker\", F.split(\"tickers\", \":\").getItem(1))\n    \n    # Generate stable article ID from link\n    .withColumn(\"article_id\", F.abs(F.hash(\"link\")) % F.lit(10**12))\n    \n    # Classify event type\n    .withColumn(\"event_type\", classify_event_type_udf(\"title\"))\n    \n    # Extract price target delta\n    .withColumn(\"price_target_delta_pct\", extract_price_target_delta_udf(\"content\"))\n    \n    # Add placeholder columns\n    .withColumn(\"earnings_surprise_pct\", F.lit(None).cast(\"double\"))\n    .withColumn(\"sentiment_score\", F.lit(0.0))\n    .withColumn(\"sentiment_label\", F.lit(\"neutral\"))\n)\n\nprint(f\"Transformed {df_transformed.count()} records\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Create Dimension Tables"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Dimension: Company\ndim_company = (df_transformed\n    .select(\"ticker\", \"exchange\")\n    .distinct()\n    .withColumn(\"company_id\", F.monotonically_increasing_id() + 1)\n    .select(\"company_id\", \"ticker\", \"exchange\")\n)\n\nprint(f\"Created dim_company with {dim_company.count()} companies\")\ndim_company.show(5, truncate=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Dimension: Event Type\ndim_event_type = (df_transformed\n    .select(\"event_type\")\n    .distinct()\n    .withColumn(\"event_type_id\", F.monotonically_increasing_id() + 1)\n    .select(\"event_type_id\", \"event_type\")\n)\n\nprint(f\"Created dim_event_type with {dim_event_type.count()} event types\")\ndim_event_type.show(truncate=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Create Fact Table"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Fact: Company Event Signal\nfact_company_event_signal = (df_transformed\n    # Join with company dimension\n    .join(dim_company, [\"ticker\", \"exchange\"], \"left\")\n    \n    # Join with event type dimension\n    .join(dim_event_type, [\"event_type\"], \"left\")\n    \n    # Add event ID\n    .withColumn(\"event_id\", F.monotonically_increasing_id() + 1)\n    \n    # Select final columns\n    .select(\n        \"event_id\",\n        \"company_id\",\n        \"event_type_id\",\n        \"article_id\",\n        \"event_date\",\n        \"sentiment_score\",\n        \"sentiment_label\",\n        \"earnings_surprise_pct\",\n        \"price_target_delta_pct\",\n        \"site\"\n    )\n)\n\nprint(f\"Created fact_company_event_signal with {fact_company_event_signal.count()} events\")\nfact_company_event_signal.show(5, truncate=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Quality Checks"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Check for null company_ids in fact table\nnull_company_ids = fact_company_event_signal.filter(F.col(\"company_id\").isNull()).count()\nif null_company_ids > 0:\n    print(f\"⚠️  Warning: {null_company_ids} events have null company_id\")\n\n# Check for null event_type_ids in fact table\nnull_event_type_ids = fact_company_event_signal.filter(F.col(\"event_type_id\").isNull()).count()\nif null_event_type_ids > 0:\n    print(f\"⚠️  Warning: {null_event_type_ids} events have null event_type_id\")\n\n# Check for duplicate event_ids\nduplicate_events = fact_company_event_signal.groupBy(\"event_id\").count().filter(F.col(\"count\") > 1).count()\nif duplicate_events > 0:\n    print(f\"⚠️  Warning: {duplicate_events} duplicate event_ids found\")\n\nprint(\"✓ Data quality checks complete\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Write to Gold Layer"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Write dimension tables\nwrite_table_idempotent(dim_company, CATALOG_NAME, TARGET_NAMESPACE, DIM_COMPANY_TABLE)\nwrite_table_idempotent(dim_event_type, CATALOG_NAME, TARGET_NAMESPACE, DIM_EVENT_TYPE_TABLE)\n\n# Write fact table\nwrite_table_idempotent(fact_company_event_signal, CATALOG_NAME, TARGET_NAMESPACE, FACT_TABLE)\n\nprint(\"\\n=== Gold Semantic Layer Created Successfully ===\")\nprint(f\"Catalog: {CATALOG_NAME}\")\nprint(f\"Namespace: {TARGET_NAMESPACE}\")\nprint(f\"Tables: {FACT_TABLE}, {DIM_COMPANY_TABLE}, {DIM_EVENT_TYPE_TABLE}\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Verification Queries"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Verify table counts\nprint(\"Table Counts:\")\nfor table in [FACT_TABLE, DIM_COMPANY_TABLE, DIM_EVENT_TYPE_TABLE]:\n    full_name = f\"`{CATALOG_NAME}`.`{TARGET_NAMESPACE}`.`{table}`\"\n    count = spark.table(full_name).count()\n    print(f\"  {table}: {count} rows\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Sample query: Top companies by event count\nprint(\"\\nTop 10 Companies by Event Count:\")\nquery = f\"\"\"\nSELECT \n    c.ticker,\n    c.exchange,\n    COUNT(*) as event_count\nFROM `{CATALOG_NAME}`.`{TARGET_NAMESPACE}`.`{FACT_TABLE}` f\nJOIN `{CATALOG_NAME}`.`{TARGET_NAMESPACE}`.`{DIM_COMPANY_TABLE}` c\n    ON f.company_id = c.company_id\nGROUP BY c.ticker, c.exchange\nORDER BY event_count DESC\nLIMIT 10\n\"\"\"\nspark.sql(query).show(truncate=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Shutdown"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Stop spark session\nfrom IPython import get_ipython as _get_ipython\n_get_ipython().user_ns[\"spark\"].stop()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
